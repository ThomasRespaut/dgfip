import os
from huggingface_hub import hf_hub_download
from llama_cpp import Llama

# Param√®tres
repo_id = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
filename = "mistral-7b-instruct-v0.1.Q5_K_M.gguf"
local_dir = "./models/mistral-7b-q5"
local_path = os.path.join(local_dir, filename)

print("üì¶ Initialisation du chargement du mod√®le Mistral-7B quantis√© (Q5_K_M)")

# √âtape 1 : Cr√©er le dossier local si besoin
if not os.path.exists(local_dir):
    print(f"üìÅ Cr√©ation du dossier local : {local_dir}")
    os.makedirs(local_dir)

# √âtape 2 : V√©rifier la pr√©sence du fichier local
if not os.path.exists(local_path):
    print("‚ùå Mod√®le non trouv√© localement.")
    print("‚¨áÔ∏è T√©l√©chargement depuis Hugging Face...")
    model_path = hf_hub_download(
        repo_id=repo_id,
        filename=filename,
        local_dir=local_dir,
        local_dir_use_symlinks=False
    )
    print(f"‚úÖ Mod√®le t√©l√©charg√© et stock√© dans : {model_path}")
else:
    print(f"‚úÖ Mod√®le d√©j√† pr√©sent localement √† : {local_path}")
    model_path = local_path

# √âtape 3 : Chargement du mod√®le avec llama.cpp
print("üöÄ Chargement du mod√®le avec llama.cpp...")
llm = Llama(
    model_path=model_path,
    n_ctx=2048
)
print("‚úÖ Mod√®le pr√™t √† l'utilisation.")

# √âtape 4 : Prompt de test
prompt = "[INST] Explique-moi le machine learning en deux phrases. [/INST]"
print("\nüß† G√©n√©ration de r√©ponse...")
output = llm(prompt, max_tokens=200)

# R√©sultat
print("\nüéØ R√©ponse g√©n√©r√©e :\n")
print(output["choices"][0]["text"])
